{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Expolortary Data Analysis\"\n",
    "subtitle: \"(EDA)\"\n",
    "author: \"Asad Raza Virk\"\n",
    "date: \"2024.09.18\"\n",
    "format: \n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "    number-sections: true\n",
    "    fig-width: 6\n",
    "    fig-height: 4\n",
    "    fig-caption: true\n",
    "    header-includes:\n",
    "      - \\usepackage{caption}\n",
    "      - \\captionsetup{font=small,labelfont=bf}\n",
    "      - \\usepackage{fancyhdr}\n",
    "      - \\pagestyle{fancy}\n",
    "      - \\fancyhead[L]{Expolortary Data Analysis}\n",
    "      - \\fancyhead[R]{\\thepage}\n",
    "      - |\n",
    "        \\fancyfoot[C]{\n",
    "          Prepared by Asad Raza Virk | Twitter: @virkat | GitHub: {https://github.com/virkat}\n",
    "        }\n",
    "      - \\usepackage{booktabs}\n",
    "      - \\usepackage{graphicx}\n",
    "      - \\usepackage{hyperref}\n",
    "    geometry: \"left=1in, right=1in, top=1in, bottom=1in\"\n",
    "    fontfamily: times\n",
    "    fontsize: 11pt\n",
    "    linestretch: 1.5\n",
    "    highlight-style: tango\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "    number-sections: true\n",
    "    fig-width: 6\n",
    "    fig-height: 4\n",
    "    fig-caption: true\n",
    "    self-contained: true\n",
    "    code-fold: true\n",
    "    include-in-header: |\n",
    "      <meta name=\"author\" content=\"Asad Raza Virk\">\n",
    "      <meta name=\"twitter\" content=\"@virkat\">\n",
    "      <meta name=\"github\" content=\"https://github.com/virkat\">\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Elements of Structured Data\n",
    "\n",
    "- Data comes from many sources: sensor measurements, events, text, images, and videos.\n",
    "\n",
    "- The Internet of Things (IoT) is spewing out streams of information. Much of this data is unstructured: images are a collection of pixels, with each pixel containing RGB (red, green, blue) color information.\n",
    "\n",
    "- Texts are sequences of words and non-word characters, often organized by sections, subsections, and so on.\n",
    "\n",
    "- Clickstreams are sequences of actions by a user interacting with an app or a web page.\n",
    "\n",
    "- A major challenge of data science is to harness this torrent of raw data into actionable information.\n",
    "\n",
    "- To apply the statistical concepts , unstructured raw data must be processed and manipulated into a structured form.\n",
    "\n",
    "One of the commonest forms of structured data is a table with rows and columns—as data might emerge from a relational database or be collected for a study\n",
    "\n",
    "## Two basic types of Structured Data\n",
    "\n",
    "1. **Numeric**\n",
    "    - continuous\n",
    "    \n",
    "        such as wind speed or time duration\n",
    "    - discrete  \n",
    "        such as the count of the occurrence of an event  \n",
    "2. **Categorical** (takes only fixed set of values)\n",
    "    - **Binary**\n",
    "        Binary data is an important special case of categorical data that takes on only one of two values, such as 0/1, yes/no, or true/false\n",
    "    - **Ordinal**\n",
    "        Ordinal data in which the categories are ordered; an example of this is a numerical rating (1, 2, 3, 4, or 5)\n",
    "\n",
    "For the purposes of data analysis and predictive modeling, the data type is important to help determine the type of visual display, data analysis, or statistical model.\n",
    "\n",
    "Data science software, such as R and Python, uses these data types to improve computational performance. More important, the data type for a variable determines how software will handle computations for that variable.\n",
    "\n",
    "## Key Terms for Data Types\n",
    "\n",
    "1.  **Numeric** Data that are expressed on a numeric scale.\n",
    "    - **Continuous** Data that can take on any value in an interval.        (Synonyms: Interval, float, numeric)\n",
    "    - **Discrete** Data that can take on only integer values, such as counts. (Synonyms: integer, count)\n",
    "2. **Categorical** Data that can take on only a specific set of values     representing a set of possible categories. (Synonyms: enums, enumerated, factors, nominal)\n",
    "    - **Binary** A special case of categorical data with just two categories of values, e.g., 0/1, true/false. \n",
    "        (Synonyms: dichotomous, logical, indicator, boolean)\n",
    "    - **Ordinal** Categorical data that has an explicit ordering. \n",
    "        (Synonym: ordered factor)\n",
    "- **Key Ideas**\n",
    "\n",
    "• Data is typically classified in software by type.\n",
    "\n",
    "• Data types include numeric (continuous, discrete) and categorical (binary, ordinal).\n",
    "\n",
    "• Data typing in software acts as a signal to the software on how to process the data.\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectangular and Non-Rectangualr Data\n",
    "\n",
    "## Rectangular Data\n",
    "\n",
    "-   The typical frame of reference for an analysis in data science is a rectangular data object, like a spreadsheet or database table.\n",
    "-   Rectangular data is the general term for a two-dimensional matrix with rows indicating records (cases) and columns indicating features (variables)\n",
    "\n",
    "-   Data frame is the specific format in R and Python.\n",
    "\n",
    "-   The data doesn’t always start in this form: unstructured data (e.g., text) must be processed and manipulated so that it can be represented as a set of features in the rectangular data\n",
    "\n",
    "-   Data in relational databases must be extracted and put into a single table for most data analysis and modeling tasks.\n",
    "\n",
    "### Key Terms for Rectangular Data\n",
    "\n",
    "1. **Data frame**\n",
    "    Rectangular data (like a spreadsheet) is the basic data structure for statistical and machine learning models.\n",
    "2.  **Feature**\n",
    "    A column within a table is commonly referred to as a feature.\n",
    "    Synonyms: attribute, input, predictor, variable\n",
    "3.  **Outcome**\n",
    "    Many data science projects involve predicting an outcome—often a yes/no out‐\n",
    "    come. The features are sometimes used to predict the outcome in an experiment or a study.\n",
    "    Synonyms: dependent variable, response, target, output\n",
    "4.  **Records**\n",
    "    A row within a table is commonly referred to as a record.\n",
    "    Synonyms: case, example, instance, observation, pattern, sample\n",
    "\n",
    "### Data Frames and Indexes\n",
    "Traditional database tables have one or more columns designated as an index, essentially a row number. This can vastly improve the efficiency of certain database queries.\n",
    "\n",
    "In Python, with the pandas library, the basic rectangular data structure is a\n",
    "DataFrame object. By default, an automatic integer index is created for a DataFrame based on the order of the rows.\n",
    "\n",
    "In pandas, it is also possible to set multilevel/hierarchical indexes to improve the efficiency of certain operations.\n",
    "\n",
    "\n",
    "> Terminology Diferences\n",
    "> Terminology for rectangular data can be confusing. Statisticians\n",
    "> and data scientists use different terms for the same thing. For a statistician, predictor variables are used in a model to predict a response or dependent variable. For a data scientist, features are used to predict a target. One synonym is particularly confusing: computer scientists will use the term sample for a single row; a sample to a statistician means a collection of rows.\n",
    "\n",
    "## Non-Rectangular Data Structures\n",
    "\n",
    "There are other data structures besides rectangular data.\n",
    "\n",
    "- **Time series data** records successive   measurements of the same variable. It is the raw material for statistical forecasting methods, and it is also a key component of the data produced by devices—the Internet of Things.\n",
    "\n",
    "- **Spatial data** structures, which are used in mapping and location analytics, are more complex and varied than rectangular data structures. In the object representation, the focus of the data is an object (e.g., a house) and its spatial coordinates. The field view, by contrast, focuses on small units of space and the value of a relevant metric (pixel brightness, for example).\n",
    "\n",
    "- **Graph (or network) data** structures are used to represent physical, social, and abstract relationships. For example, a graph of a social network, such as Facebook or LinkedIn, may represent connections between people on the network. Distribution hubs connected by roads are an example of a physical network. Graph structures are useful for certain types of problems, such as network optimization and recommender systems.\n",
    "\n",
    "    > Graphs in Statistics\n",
    "    > In computer science and information technology, the term graph\n",
    "    > typically refers to a depiction of the connections among entities,\n",
    "    > and to the underlying data structure. In statistics, graph is used to\n",
    "    > refer to a variety of plots and visualizations, not just of connections\n",
    "    > among entities, and the term applies only to the visualization, not\n",
    "    > to the data structure.\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimates of Location of Data\n",
    "- Variables with measured or count data might have thousands of distinct values.\n",
    "- A basic step in exploring your data is getting a “typical value” for each feature (variable): **an estimate of where most of the data is located** (i.e., its central tendency).\n",
    "\n",
    "## Key Terms for Estimates of Location\n",
    "- **Mean**\n",
    "    - The sum of all values divided by the number of values.\n",
    "        - Synonym\n",
    "            - average\n",
    "- **Weighted mean**\n",
    "    - The sum of all values times a weight divided by the \n",
    "    sum of the weights.\n",
    "        - Synonym\n",
    "            - weighted average\n",
    "- **Median**\n",
    "    - The value such that one-half of the data lies above and below.\n",
    "        - Synonym\n",
    "            - 50th percentile\n",
    "- **Percentile**\n",
    "    - The value such that P percent of the data lies below.\n",
    "        - Synonym\n",
    "            - quantile\n",
    "- **Weighted median**\n",
    "    - The value such that one-half of the sum of the weight\n",
    "- **Trimmed mean**\n",
    "    - The average of all values after dropping a fixed number of extreme values.\n",
    "        - Synonym\n",
    "            - truncated mean\n",
    "- **Robust**\n",
    "    - Not sensitive to extreme values.\n",
    "        - Synonym\n",
    "            - resistant\n",
    "- **Outlier**\n",
    "    - A data value that is very different from most of the data.\n",
    "        - Synonym\n",
    "            - extreme value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Metrics and Estimates** \n",
    "\n",
    "\n",
    "**Statisticians** often use the term **estimate** for a value calculated from\n",
    "the data at hand, to draw a distinction between what we see from\n",
    "the data and the theoretical true or exact state of affairs. **Data scientists and business analysts** are more likely to refer to such a value as a **metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Mean**\n",
    "###  Mean\n",
    "- The mean is the sum of all values divided by the number of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mean](01_00_mean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed Mean\n",
    "- A variation of the mean which calculate the mean by dropping a fixed number of sorted values at each end and then taking an average of the remaining values\n",
    "- A trimmed mean eliminates the influence of extreme values\n",
    "- The trimmed mean is a robust measure of central tendency, as it is less affected by outliers than the mean\n",
    "- The trimmed mean is calculated by first sorting the data in ascending order, then dropping a fixed number\n",
    "- For example, in international diving the top score and bottom score from five judges are dropped, and the final score is the average of the scores from the three remaining judges. This makes it difficult for a single judge to manipulate the score, perhaps to favor their country’s\n",
    "contestant.\n",
    "- Trimmed means are widely used, and in many cases are preferable to using the ordinary mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Trimmed Mean](01_00_trimmed_mean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Trimmed Mean \n",
    "(Mean = Red, Trimmed Mean = Greed)\n",
    "![Mean and Trimmed Mean](01_00_mean_and_trimmed_mean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outliers**\n",
    "- outliers (extreme cases) could skew the results\n",
    "- An outlier is any value that is very distant from the other values in a data set\n",
    "- The exact definition of an outlier is somewhat subjective\n",
    "- Outliers can be either high or low values\n",
    "- Being an outlier in itself does not make a data value invalid or erroneous\n",
    "- outliers are often the result of data errors such as mixing data of different units (kilometers versus meters) or bad readings from a sensor.\n",
    "- When outliers are the result of bad data, the mean will result in a poor estimate of location, while the median will still be valid\n",
    "- In any case, outliers should be identified and are usually\n",
    "worthy of further investigation.\n",
    "\n",
    "> **Anomaly Detection**\\\n",
    "In contrast to typical data analysis, where outliers are sometimes informative and sometimes a nuisance, in anomaly detection the points of interest are the outliers, and the greater mass of data serves primarily to define the “normal” against which anomalies are measured.\n",
    "\n",
    "\n",
    "The **median** is not the only robust estimate of location. In fact, a **trimmed mean** is widely used to avoid the influence of outliers. For example, trimming the bottom and top 10% (a common choice) of the data will provide protection against outliers in all but the smallest data sets. The **trimmed mean** can be thought of as a compromise between the median and the mean: it is robust to extreme values in the data, but uses more data to calculate the estimate for location.\n",
    "\n",
    "\n",
    "> Weighted mean is available with NumPy. For weighted median, we can use the specialized package wquantiles\n",
    "\n",
    "### **Key Ideas**\n",
    "- The basic metric for location is the mean, but it can be sensitive to extreme values (outlier).\n",
    "- Other metrics (median, trimmed mean) are less sensitive to outliers and unusual distributions and hence are more robust\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimates of Variability\n",
    "\n",
    "##  **Variability**\n",
    "- Location is just one dimension in summarizing a feature\n",
    "- A second dimension, variability, also referred to as dispersion, measures whether the data values are tightly clustered or spread out.\n",
    "- At the heart of statistics lies variability:\n",
    "    - measuring it\n",
    "    - reducing it\n",
    "    - distinguishing random from real variability \n",
    "    - identifying the various sources of real variability\n",
    "    - making decisions in the presence of it\n",
    "- Variability is often measured using the range, interquartile range (IQR), variance, or standard deviation\n",
    "\n",
    "### **Key Terms for Variability Metrics**\n",
    "- **Deviations**\n",
    "    - The difference between the observed values and the estimate of location.\n",
    "        - Synonyms\n",
    "            - errors, residuals\n",
    "- **Variance**\n",
    "    - The sum of squared deviations from the mean divided by n – 1 where n is the number of data values.\n",
    "        - Synonym\n",
    "            - mean-squared-error (MSE)\n",
    "\n",
    "    ![Example](04_1_variance.png)\n",
    "\n",
    "- **Standard deviation**\n",
    "    - The square root of the variance.\n",
    "        - Synonyms\n",
    "            - root mean squared error (RMSE) \n",
    "- **Mean absolute deviation**\n",
    "    - The mean of the absolute values of the deviations from the mean.\n",
    "        - Synonyms\n",
    "            - l1-norm, Manhattan norm   \n",
    "\n",
    "    ![Example](04_2_Mean_Absolute_Deviation.png)\n",
    "- **Median absolute deviation from the median**\n",
    "    -   The median of the absolute values of the deviations from the median\n",
    "    \n",
    "    ![Example](04_3_mad_from_median.jpg)\n",
    "- **Range**\n",
    "    -   The difference between the largest and the smallest value in a data set.\n",
    "- **Order statistics**\n",
    "    -   Metrics based on the data values sorted from smallest to biggest.\n",
    "        - Synonym\n",
    "            -   ranks\n",
    "- **Percentile**\n",
    "    - The value such that P percent of the values take on this value or less and (100–P) percent take on this value or more.\n",
    "        - Synonym\n",
    "            - quantile\n",
    "- **Interquartile range**\n",
    "    - The difference between the 75th percentile and the 25th percentile.\n",
    "        - Synonym\n",
    "            - IQR\n",
    "\n",
    "## **Standard Deviation and Related Estimates**\n",
    "- The most widely used estimates of variation are based on the differences, or **deviations**, between the estimate of location and the observed data.\n",
    "- For a set of data {1, 4, 4}, the mean is 3 and the median is 4. The deviations from the mean are the differences: 1 – 3 = –2, 4 – 3 = 1, 4 – 3 = 1.\n",
    "- These deviations tell us how dispersed the\n",
    "data is around the central value\n",
    "- One way to measure variability is to estimate a typical value for these deviations.Averaging the deviations themselves would not tell us much—the negative deviations offset the positive ones. In fact, the sum of the deviations from the mean is precisely zero.\n",
    "-  Instead, a simple approach is to take the average of the absolute values of the deviations from the mean. In the preceding example, the absolute value of the deviations is {2 1 1}, and their average is (2 + 1 + 1) / 3 = 1.33. This is known as the **mean absolute deviation** \n",
    "- The best-known estimates of variability are the _**variance**_ and the _**standard deviation**_, which are based on _**squared deviations**_. **The variance is an average of the squared deviations, and the standard deviation is the square root of the variance**\n",
    "- The standard deviation is much easier to interpret than the variance since it is on the same scale as the original data. Still, with its more complicated and less intuitive formula, it might seem peculiar that the standard deviation is preferred in statistics over the mean absolute deviation. It owes its preeminence to statistical theory: mathematically, working with squared values is much more convenient than absolute values,\n",
    "especially for statistical models.\n",
    "\n",
    "\n",
    "> here is always some discussion of why we have n – 1 in the denominator in the variance formula, instead of n, leading into the concept of degreesof freedom. This distinction is not important since n is generally large enough that it won’t make much difference whether you divide by n or n – 1. But in case you are interested, here is the story. It is based on the premise that you want to make estimates about a population, based on a sample.\n",
    "> If you use the intuitive denominator of n in the variance formula, you will underestimate the true value of the variance and the standard deviation in the population. This is referred to as a **biased estimate**. However, if you divide by n – 1 instead of n, the variance becomes an **unbiased estimate.**\n",
    "> To fully explain why using n leads to a biased estimate involves the notion of degrees of freedom, which takes into account the number of constraints in computing an estimate. In this case, there are n – 1 degrees of freedom since there is one constraint: the standard deviation depends on calculating the sample mean. For most problems, data scientists do not need to worry about degrees of freedom.\n",
    "\n",
    "- Neither the variance, the standard deviation, nor the mean absolute deviation is robust to outliers and extreme values\n",
    "- The variance and standard deviation are especially sensitive to outliers since they are based on the squared deviations.\n",
    "- A robust estimate of variability is the **median absolute deviation from the median or MAD**\n",
    "- Like the median, the MAD is not influenced by extreme val‐\n",
    "ues.\n",
    "- It is also possible to compute a trimmed standard deviation analogous to the trimmed mean\n",
    "> The variance, the standard deviation, the mean absolute deviation, and the median absolute deviation from the median are not equivalent estimates, even in the case where the data comes from a normal distribution. In fact, the standard deviation is always greater than the mean absolute deviation, which itself is greater than the median absolute deviation. Sometimes, the median absolute deviation is multiplied by a constant scaling factor to put the MAD on the same scale as the standard deviation in the case of a normal distribution. The commonly used factor of 1.4826 means that 50% of the normal distribution fall within the range ±MAD\n",
    "\n",
    "\n",
    "The purpose of calculating deviations in the values is to understand how each individual value differs from the average (mean) value. This helps in several ways:\n",
    "\n",
    "- **Measure of Variability**: Deviations provide a measure of the spread or variability in the data. Large deviations indicate that the charges are spread out over a wide range, while small deviations suggest that the charges are clustered closely around the mean.\n",
    "\n",
    "- **Identify Outliers:** By examining the deviations, you can identify outliers or unusual data points that are significantly different from the mean.\n",
    "\n",
    "- **Statistical Analysis:** Deviations are a fundamental component in various statistical analyses, such as calculating the variance and standard deviation, which are key measures of data dispersion.\n",
    "\n",
    "- **Data Visualization:** Plotting the deviations can help visualize the distribution of the data, making it easier to identify patterns, trends, and anomalies.\n",
    "\n",
    "In statistical analysis, variance and deviation are closely related concepts that measure the spread or dispersion of a dataset. Here’s how they are related:\n",
    "\n",
    "- Deviation:\n",
    "\n",
    "    - Deviation refers to the difference between each data point and the mean of the dataset.\n",
    "    - Deviations can be positive or negative, depending on whether the data point is above or below the mean.\n",
    "    - The sum of deviations from the mean is always zero, as positive deviations cancel out negative deviations\n",
    "\n",
    "- Variance:\n",
    "\n",
    "    - Variance is a measure of how much the data points in a dataset vary from the mean.\n",
    "    - It is calculated as the average of the squared deviations from the mean.\n",
    "    - Squaring the deviations ensures that all values are positive and gives more weight to larger deviations.\n",
    "    - The variance is always non-negative and is zero if all data points are identical\n",
    "\n",
    "- Relationship:\n",
    "\n",
    "    - Variance is essentially the mean of the squared deviations.\n",
    "    - While deviations provide a measure of individual differences from the mean, variance provides a single value that summarizes the overall dispersion of the dataset.\n",
    "    - Variance is used to calculate the standard deviation, which is the square root of the variance and provides a measure of dispersion in the same units as the original data.\n",
    "    \n",
    "In summary, deviations are the building blocks for calculating variance, and variance provides a comprehensive measure of the spread of the data based on these deviations.\n",
    "\n",
    "This code will output a table with the columns for value, mean, deviation, and variance, and it will also plot the deviations. Adjust the df['charges'] list with your actual data if needed.\n",
    "\n",
    "\n",
    "![Example](01_00_multi_figs.png)\n",
    "\n",
    "The variance is a single value that summarizes the overall dispersion of the dataset. It is not specific to individual data points but rather describes the dataset as a whole. Therefore, when you include the variance in the DataFrame, it is the same for every row because it represents the same overall measure of variability for the entire dataset.\n",
    "\n",
    "If you want to include the variance in the DataFrame, it should be shown as a single value, not repeated for each data point. However, if you want to show the squared deviations (which contribute to the variance), you can include those instead.\n",
    "\n",
    "In statistical analysis, the standard deviation and variance are both measures of the spread or dispersion of a dataset. They are closely related but differ in how they express this dispersion.\n",
    "\n",
    "- **Variance**\n",
    "- Definition: Variance measures the average squared deviations from the mean.\n",
    "- Units: The units of variance are the square of the units of the original data. For example, if the data is in meters, the variance will be in square meters.\n",
    "- Formula: Variance = Σ(xi - μ)^2 / (n - 1), where \n",
    "    - xi is each data point,\n",
    "    - μ is the mean of the dataset,\n",
    "    - n is the number of data points, and\n",
    "    - Σ denotes the sum of the squared deviations.\n",
    "\n",
    "- **Standard Deviation**\n",
    "- Definition: Standard deviation is the square root of the variance. It provides a measure of dispersion in the same units as the original data.\n",
    "\n",
    "- Units: The units of standard deviation are the same as the units of the original data. For example, if the data is in meters, the standard deviation will also be in meters.\n",
    "\n",
    "- **Relationship**\n",
    "Mathematical Relationship: The standard deviation is the square root of the variance.\n",
    "- Interpretation:\n",
    "    - Variance gives a measure of how data points spread out from the mean, but because it uses squared units, it can be less intuitive.\n",
    "    - Standard deviation, being in the same units as the data, is often more interpretable and is commonly used to describe the spread of the data.\n",
    "- Example\n",
    "If you have a dataset, the variance tells you the average of the squared differences from the mean, while the standard deviation tells you how much the data points typically deviate from the mean in the original units of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates Based on Percentiles\n",
    "A different approach to estimating dispersion is based on looking at the spread of the sorted data.Statistics based on sorted (ranked) data are referred to as **order statistics**.\n",
    "\n",
    "- The most basic measure is the range: the difference between the largest and smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers, but the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.\n",
    "\n",
    "- To avoid the sensitivity to outliers, we can look at the range of the data after dropping values from each end Formally, these types of estimates are based on differences between **percentiles**\n",
    "\n",
    "- In a data set, the Pth percentile is a value such that at least P percent of the values take on this value or less and at least (100 – P) percent of the values take on this value or more. For example, to find the 80th percentile, sort the data. Then, starting with the smallest value, proceed 80 percent of the way to the largest value. Note that the median is the same thing as the 50th percentile. The percentile is essentially the same as a quantile, with quantiles indexed by fractions (so the .8 quantile is the same as the 80th percentile).\n",
    "\n",
    "- A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called the interquartile range (or IQR).\n",
    "\n",
    "-  Software can have slightly differing approaches that yield different answers\n",
    " \n",
    "- For very large data sets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values. Machine learning and statistical software use special algorithms, such as [Zhang-Wang-2007], to get an approximate percentile that can be calculated very quickly and is guaranteed to have a certain accuracy.\n",
    "\n",
    "![Range](01_00_range.png)\n",
    "\n",
    "Order statistics are metrics based on the data values sorted from smallest to largest. They provide insights into the distribution and spread of the data. Here are some common order statistics:\n",
    "\n",
    "- Minimum: The smallest value in the dataset.\n",
    "- Maximum: The largest value in the dataset.\n",
    "- Median: The middle value when the data is sorted. If the dataset has an even number of observations, the median is the average of the two middle values.\n",
    "- Quartiles: Values that divide the dataset into four equal parts.\n",
    "    - First Quartile (Q1): The median of the lower half of the dataset (25th percentile).\n",
    "    - Second Quartile (Q2): The median of the dataset (50th percentile).\n",
    "    - Third Quartile (Q3): The median of the upper half of the dataset (75th percentile).\n",
    "- Percentiles: Values that divide the dataset into 100 equal parts. For example, the 90th percentile is the value below which 90% of the data falls.\n",
    "\n",
    "![Percentile](01_00_order_stat.png)\n",
    "\n",
    "\n",
    "\n",
    "![IQR](01_00_order_stat.png)\n",
    "\n",
    "\n",
    "### **Key Ideas**\n",
    "• Variance and standard deviation are the most widespread and routinely reported statistics of variability.\n",
    "\n",
    "• Both are sensitive to outliers.\n",
    "\n",
    "• More robust metrics include mean absolute deviation, median absolute deviation from the median, and percentiles (quantiles).\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data Distribution\n",
    "\n",
    "Each of the estimates sums up the data in a single number to describe\n",
    "the location or variability of the data. It is also useful to explore how the data is distributed overall.\n",
    "\n",
    "## Key Terms for Exploring the Distribution\n",
    "\n",
    "- Boxplot\n",
    "    - A plot introduced by Tukey as a quick way to visualize the distribution of data.\n",
    "        - Synonym\n",
    "            - box and whiskers plot\n",
    "- Frequency table\n",
    "    - A tally of the count of numeric data values that fall into a set of intervals (bins).\n",
    "- Histogram\n",
    "    - A plot of the frequency table with the bins on the x-axis and the count (or pro‐\n",
    "    portion) on the y-axis. While visually similar, bar charts should not be confused with histograms.\n",
    "- Density plot\n",
    "    - A smoothed version of the histogram, often based on a kernel density estimate.\n",
    "\n",
    "\n",
    "## Percentiles and Boxplots\n",
    "\n",
    "### Percentiles\n",
    "\n",
    "Percentiles can be used to measure the spread of the data. Percentiles are also valuable for summarizing the entire distribution. It is common to report the quartiles (25th, 50th, and 75th per‐centiles) and the deciles (the 10th, 20th, …, 90th percentiles). Percentiles are especially valuable for summarizing the tails (the outer range) of the distribution. Popular culture has coined the term one-percenters to refer to the people in the top 99th percentile of wealth.\n",
    "\n",
    "![Percentile](01_00_percent.png)\n",
    "\n",
    "\n",
    "### Boxplots\n",
    "Boxplots introduced by Tukey [Tukey-1977], are based on percentiles and give a\n",
    "quick way to visualize the distribution of data.\n",
    "\n",
    "![Boxplot](01_00_boxplot.png)\n",
    "\n",
    "- The top and bottom of the box are the 75th and 25th percentiles, respectively\n",
    "- The median is shown by the horizontal line in the box\n",
    "- The whiskers are the two lines outside the box that extend to the highest and lowest observations that are within 1.5 * IQR from the upper and lower quartiles\n",
    "- Any points outside this range are plotted as individual points\n",
    "- There are many variations of a boxplot\n",
    "- By default, the R function extends the whiskers to the furthest point beyond the box, except that it will not go beyond 1.5 times the IQR. Matplotlib uses the same implementation; other software may use a different rule.\n",
    "- Any data outside of the whiskers is plotted as single points or circles (often considered outliers).\n",
    "- Pandas provides a number of basic exploratory plots for data frame; one of them is\n",
    "boxplots\n",
    "\n",
    "\n",
    "## Frequency Tables and Histograms\n",
    "\n",
    "### Frequency Tables\n",
    "- A frequency table of a variable divides up the variable range into equally spaced segments and tells us how many values fall within each segment. \n",
    "- The function pandas.cut creates a series that maps the values into the segments.\n",
    "Using the method value_counts, we get the frequency table\n",
    "\n",
    "![Frequency Table](01_00_frequency_table.png)\n",
    "\n",
    "> It is important to include the empty bins; the fact that there are no values\n",
    ">in those bins is useful information. It can also be useful to experiment with different bin sizes. If they are too large, important features of the distribution can be obscured.If they are too small, the result is too granular, and the ability to see the bigger picture is lost.\n",
    "\n",
    "> Both frequency tables and percentiles summarize the data by creat‐ing bins. In general, quartiles and deciles will have the same count in each bin (equal-count bins), but the bin sizes will be different. The frequency table, by contrast, will have different counts in the bins (equal-size bins), and the bin sizes will be the same.\n",
    "\n",
    "### Histogram\n",
    "A histogram is a way to visualize a frequency table, with bins on the x-axis and the\n",
    "data count on the y-axis.\n",
    "Pandas supports histograms for data frames with the DataFrame.plot.hist method.\n",
    "Use the keyword argument bins to define the number of bins. The various plot meth‐\n",
    "ods return an axis object that allows further fine-tuning of the visualization using\n",
    "Matplotlib:\n",
    "\n",
    "![Histogram](01_00_hist.png)\n",
    "\n",
    "\n",
    "Histograms are plotted such that:\n",
    "\n",
    "• Empty bins are included in the graph.\n",
    "\n",
    "• Bins are of equal width.\n",
    "\n",
    "• The number of bins (or, equivalently, bin size) is up to the user.\n",
    "\n",
    "• Bars are contiguous—no empty space shows between bars, unless there is an\n",
    "empty bin.\n",
    "\n",
    "\n",
    "> Statistical Moments\n",
    "\n",
    "> In statistical theory, _location_ and _variability_ are referred to as the first and second moments of a **distribution.** The third and fourth moments are called _skewness_ and _kurtosis_. Skewness refers to whether the data is skewed to larger or smaller values, and kurtosis indicates the propensity of the data to have extreme values. Generally, metrics are not used to measure skewness and kurtosis; instead, these are discovered through visual displays\n",
    "\n",
    "## Density Plots and  Estimates\n",
    "\n",
    "- Related to the histogram is a density plot, which shows the distribution of data values as a continuous line.\n",
    "- A density plot can be thought of as a smoothed histogram, although it is typically computed directly from the data through a kernel density estimate \n",
    "- pandas provides the density method to create a density plot. Use the argument\n",
    "bw_method to control the smoothness of the density curve\n",
    "\n",
    "![Density Plot](01_00_den.png)\n",
    "\n",
    "A key distinction from the histogram is the scale of the y-axis: a density plot corresponds to plotting the histogram as a proportion rather than counts. Note that the total area under the density curve = 1, and instead of counts in bins you calculate areas under the curve between any two points on the x-axis, which correspond to the proportion of the distribution lying between those two points\n",
    "\n",
    "> Density Estimation\n",
    "\n",
    "> Density estimation is a rich topic with a long history in statistical literature. The density estimation methods in pandas and scikit-learn also offer good implementations. For many data science problems, there is no need to worry about the various types of density estimates; it suffices to use the base functions.\n",
    "\n",
    "## Key Ideas\n",
    "- A frequency histogram plots frequency counts on the y-axis and variable values\n",
    "on the x-axis; it gives a sense of the distribution of the data at a glance.\n",
    "- A frequency table is a tabular version of the frequency counts found in a\n",
    "histogram.\n",
    "- A boxplot—with the top and bottom of the box at the 75th and 25th percentiles,\n",
    "respectively—also gives a quick sense of the distribution of the data; it is often\n",
    "used in side-by-side displays to compare distributions.\n",
    "- A density plot is a smoothed version of a histogram; it requires a function to esti‐\n",
    "mate a plot based on the data (multiple estimates are possible, of course)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Exploring Binary and Categorical Data\n",
    "\n",
    "For categorical data, simple proportions or percentages tell the story of the data. For binary data, the proportion of 1s (or 0s) is often the most important metric. For both types of data, the mode (the most common value) is often the most informative summary statistic.\n",
    "\n",
    "## Key Terms for Exploring Categorical Data\n",
    "\n",
    "- **Mode**\n",
    "    - The most commonly occurring category or value in a data set.\n",
    "- **Frequency**\n",
    "    - The number of times a value occurs in a data set.\n",
    "- **Proportion**\n",
    "    - The fraction of the data that takes on a particular value.\n",
    "- **Bar chart**\n",
    "    - A plot of the frequency or proportion for each category of a categorical variable.\n",
    "- **Pie chart**\n",
    "    - A plot that shows the proportion of cases that fall into each category of a categorical variable.\n",
    "- Expected value\n",
    "    - When the categories can be associated with a numeric value, this gives an average value based on a category’s probability of occurrence\n",
    "    - Mean of a probability distribution\n",
    "\n",
    "Getting a summary of a binary variable or a categorical variable with a few categories is a fairly easy matter: we just figure out the proportion of 1s, or the proportions of the important categories.\n",
    "\n",
    "![Percentage of Smokers](01_00_percentage_table.png)\n",
    "\n",
    "\n",
    "## Bar Charts\n",
    "\n",
    "- A bar chart is a common way to represent categorical data. The height of each bar represents the frequency (or proportion) of each category.\n",
    "\n",
    "- A common visual tool for displaying a single categorical variable\n",
    "\n",
    "- Categories are listed on the x-axis, and frequencies or pro‐\n",
    "portions on the y-axis\n",
    "\n",
    "![Bar Chart](01_00_bar.png)\n",
    "\n",
    "- The bar chart is a simple and effective way to visualize categorical data. It is especially useful when the number of categories is small, and the data is not too granular. For larger numbers of categories, the chart can become cluttered and difficult to interpret.\n",
    "\n",
    "Note that a bar chart resembles a histogram; in a bar chart the x-axis represents dif‐\n",
    "ferent categories of a factor variable, while in a histogram the x-axis represents values\n",
    "of a single variable on a numeric scale. In a histogram, the bars are typically shown\n",
    "touching each other, with gaps indicating values that did not occur in the data. In a\n",
    "bar chart, the bars are shown separate from one another.\n",
    "\n",
    "## Pie Charts\n",
    "\n",
    "- A pie chart is another way to visualize the distribution of a categorical variable. The size of each slice of the pie represents the proportion of each category.\n",
    "\n",
    "- Pie charts are often used to show the relative sizes of the categories in a categorical variable\n",
    "\n",
    "- The pie chart is a common way to visualize the distribution of a single categorical variable. It is especially useful when the number of categories is small and the data is not too granular. For larger numbers of categories, the chart can become cluttered and difficult to interpret.\n",
    "\n",
    "- Pie charts are often criticized for being difficult to interpret, especially when there are many categories or when the categories are not ordered by size. In these cases, a bar chart is often a better choice.\n",
    "\n",
    "![Pie Chart](01_00_pie.png)\n",
    "\n",
    "\n",
    "Pie charts are an alternative to bar charts, although statisticians and data visualization\n",
    "experts generally eschew pie charts as less visually informative.\n",
    "\n",
    "\n",
    "**Numerical Data as Categorical Data**\n",
    "\n",
    "The frequency tables are based on binning the data. This implicitly converts the numeric data to an ordered factor. In this sense, histograms and bar charts are similar, except that the categories on the x-axis in the\n",
    "bar chart are not ordered. Converting numeric data to categorical data is an important and widely used step in data analysis since it\n",
    "reduces the complexity (and size) of the data. This aids in the discovery of relationships between features, particularly at the initial\n",
    "stages of an analysis.\n",
    "\n",
    "## Mode\n",
    "\n",
    "- The mode is the value—or values in case of a tie—that appears most often in the data.\n",
    "\n",
    "- The mode is a simple summary statistic for\n",
    "categorical data, and it is generally not used for numeric data.\n",
    "\n",
    "- The mode is especially useful for understanding the central tendency of categorical data, and it is often used in conjunction with bar charts and pie charts.\n",
    "\n",
    "## Expected Value\n",
    "\n",
    "- A special type of categorical data is data in which the categories represent or can be\n",
    "mapped to discrete values on the same scale.\n",
    "\n",
    "- In this case, the expected value is the average value of the variable, weighted by the probability of each category.\n",
    "\n",
    "- The expected value is really a form of weighted mean: it adds the ideas of future\n",
    "expectations and probability weights, often based on subjective judgment. Expected\n",
    "value is a fundamental concept in business valuation and capital budgeting.\n",
    "\n",
    "\n",
    "## Probability\n",
    "\n",
    "- The probability of a value occurring. Most people have an intuitive understanding of probability. For example, the probability of a fair coin landing heads is 0.5.\n",
    "\n",
    "- In data science, probability is used to model uncertainty and randomness in data. It is a key concept in statistical inference, machine learning, and decision-making under uncertainty.\n",
    "\n",
    "- Probability is often used to estimate the likelihood of an event occurring, given certain conditions or assumptions. It is expressed as a value between 0 and 1, where 0 indicates impossibility and 1 indicates certainty.\n",
    "\n",
    "- In data analysis, probability is used to model the likelihood of different outcomes, such as the probability of a customer making a purchase, the probability of a machine failing, or the probability of a patient having a particular disease.\n",
    "\n",
    "- Probability theory provides a mathematical framework for analyzing random events and making predictions based on data. It is a fundamental concept in data science and is used in various statistical models, such as Bayesian inference, logistic regression, and decision trees.\n",
    "\n",
    "- Probability is also used to calculate expected values, which represent the average outcome of a random variable based on its probability distribution. Expected values are used in decision-making, risk assessment, and optimization problems.   \n",
    "\n",
    "- In summary, probability is a key concept in data science that is used to model uncertainty, make predictions, and analyze random events. It provides a foundation for statistical inference, machine learning, and decision-making under uncertainty.\n",
    "\n",
    "## Key Ideas\n",
    "- For binary and categorical data, the mode is often the most informative summary statistic.\n",
    "\n",
    "- Bar charts and pie charts are common ways to visualize categorical data.\n",
    "\n",
    "- Expected value is a useful concept when the categories can be mapped to discrete values.\n",
    "\n",
    "- Categorical data is typically summed up in proportions and can be visualized in a\n",
    "bar chart.\n",
    "\n",
    "- Categories might represent distinct things (apples and oranges, male and female),\n",
    "levels of a factor variable (low, medium, and high), or numeric data that has been\n",
    "binned.\n",
    "\n",
    "- Expected value is the sum of values times their probability of occurrence, often\n",
    "used to sum up factor variable levels.\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "- Correlation is a measure of the strength and direction of the relationship between two variables. It is a key concept in statistics and data analysis, as it helps to identify patterns, trends, and associations in the data.\n",
    "\n",
    "- Correlation is often used to determine whether two variables are related and to what extent. It is commonly used in predictive modeling, hypothesis testing, and feature selection.\n",
    "\n",
    "- Exploratory data analysis in many modeling projects (whether in data science or in\n",
    "research) involves examining correlation among predictors, and between predictors\n",
    "and a target variable. Variables X and Y (each with measured data) are said to be posi‐\n",
    "tively correlated if high values of X go with high values of Y, and low values of X go\n",
    "with low values of Y. If high values of X go with low values of Y, and vice versa, the\n",
    "variables are negatively correlated.\n",
    "\n",
    "- Correlation is a statistical measure that ranges from -1 to 1. A correlation of 1 indicates a perfect positive relationship, a correlation of -1 indicates a perfect negative relationship, and a correlation of 0 indicates no relationship between the variables.\n",
    "\n",
    "![Correlation](01_00_corr.png)\n",
    "\n",
    "- The correlation coefficient is a measure of the strength and direction of the relationship between two variables. It ranges from -1 to 1, where:\n",
    "    - 1 indicates a perfect positive relationship,\n",
    "    - -1 indicates a perfect negative relationship, and\n",
    "    - 0 indicates no relationship between the variables.\n",
    "\n",
    "\n",
    "## Key Terms for Correlation\n",
    "\n",
    "- Correlation coefficient\n",
    "    A metric that measures the extent to which numeric variables are associated with one another (ranges from –1 to +1).\n",
    "\n",
    "- Correlation matrix\n",
    "    A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.\n",
    "- Scatterplot\n",
    "    A plot in which the x-axis is the value of one variable, and the y-axis the value of another.\n",
    "- Covariance\n",
    "    A measure of how changes in one variable are associated with changes in a second variable.\n",
    "\n",
    "## Correlation Coefficient\n",
    "\n",
    "- More useful is a standardized variant\n",
    "\n",
    "- Gives an estimate of the correlation between two variables that always lies on the same scale\n",
    "\n",
    "- To compute Pearson’s correlation coefficient, we multiply deviations from the mean for variable 1 times those for variable 2, and divide by the product of the standard deviations for the two variables\n",
    "\n",
    "- The correlation coefficient always lies between +1 (perfect positive correlation) and –1 (perfect negative correlation); 0 indicates no correlation.\n",
    "\n",
    "- Variables can have an association that is not linear, in which case the correlation coefficient may not be a useful metric.\n",
    "\n",
    "- A table of correlation coefficients for all pairs of variables is called a correlation matrix.\n",
    "\n",
    "- The correlation matrix is a square matrix that shows the correlation coefficients between all pairs of variables in a dataset. It is a useful tool for identifying relationships between variables and understanding the structure of the data.\n",
    "\n",
    "- The correlation matrix is often used in exploratory data analysis to identify patterns, trends, and associations in the data. It can help to identify which variables are related and to what extent, which is useful for feature selection, predictive modeling, and hypothesis testing.\n",
    "\n",
    "- The correlation matrix is a key tool in data analysis and is commonly used in statistics, machine learning, and data science to understand the relationships between variables in a dataset.\n",
    "\n",
    "![Correlation Matrix HeatMap](01_00_heatmap.png)\n",
    "\n",
    "\n",
    "- Like the mean and standard deviation, the correlation coefficient is sensitive to outliers in the data\n",
    "\n",
    "- Software packages offer robust alternatives to the classical correlation coefficient that are less sensitive to outliers.\n",
    "\n",
    "- The methods in the scikit-learn module sklearn.covariance implement a variety of approaches\n",
    "\n",
    "**Other Correlation Estimates**\n",
    "\n",
    "Statisticians long ago proposed other types of correlation coefficients, such as Spearman’s rho or Kendall’s tau. These are correlation coefficients based on the rank of the data. Since they work with ranks rather than values, these estimates are robust to outliers and can handle certain types of nonlinearities. However, data scientists can generally stick to Pearson’s correlation coefficient, and its robust alternatives, for exploratory analysis. The appeal of rankbased estimates is mostly for smaller data sets and specific hypothesis tests.\n",
    "\n",
    "\n",
    "## Scatterplot\n",
    "\n",
    "- The standard way to visualize the relationship between two measured data variables is with a scatterplot. The x-axis represents one variable and the y-axis another, and each point on the graph is a record.\n",
    "\n",
    "- A scatterplot is a visual representation of the relationship between two variables. It is commonly used in data analysis to identify patterns, trends, and associations in the data.\n",
    "\n",
    "- Scatterplots are useful for visualizing the relationship between two continuous variables. They help to identify correlations, outliers, and nonlinear relationships in the data.\n",
    "\n",
    "- Scatterplots are often used in exploratory data analysis to understand the structure of the data and to identify potential relationships between variables.\n",
    "\n",
    "![Scatterplot](01_00_scaplot.png)\n",
    "\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- The correlation coefficient measures the extent to which two paired variables (e.g., height and weight for individuals) are associated with one another.\n",
    "\n",
    "- When high values of v1 go with high values of v2, v1 and v2 are positively\n",
    "associated.\n",
    "\n",
    "- When high values of v1 go with low values of v2, v1 and v2 are negatively\n",
    "associated.\n",
    "\n",
    "- The correlation coefficient is a standardized metric, so that it always ranges from –1 (perfect negative correlation) to +1 (perfect positive correlation).\n",
    "\n",
    "- A correlation coefficient of zero indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the  correlation coefficient just by chance.\n",
    "\\newpage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Two or More Variables\n",
    "\n",
    "- Estimators like mean and variance look at variables one at a time  (univariate analysis)\n",
    "\n",
    "- Correlation analysis is an important method that compares two variables (bivariate analysis)\n",
    "\n",
    "- In practice, we often need to look at more than two variables at a time (multivariate analysis)\n",
    "\n",
    "- Multivariate analysis is a key part of data analysis and is used to identify patterns, trends, and relationships between multiple variables in a dataset.\n",
    "\n",
    "- Multivariate analysis is used in various fields, such as statistics, machine learning, and data science, to understand complex relationships between variables and to make predictions based on multiple factors.\n",
    "\n",
    "## Key Terms for Exploring Two or More Variables\n",
    "\n",
    "- Contingency table\n",
    "    A tally of counts between two or more categorical variables.\n",
    "\n",
    "- Hexagonal binning\n",
    "    A plot of two numeric variables with the records binned into hexagons.\n",
    "\n",
    "- Contour plot\n",
    "    A plot showing the density of two numeric variables like a topographical map.\n",
    "\n",
    "- Violin plot\n",
    "    Similar to a boxplot but showing the density estimate.\n",
    "\n",
    "Like univariate analysis, bivariate analysis involves both computing summary statistics and producing visual displays. The appropriate type of bivariate or multivariate\n",
    "analysis depends on the nature of the data: **numeric versus categorical**.\n",
    "\n",
    "## Hexagonal Binning and Contours(Plotting Numeric Versus Numeric Data)\n",
    "\n",
    "- When you have a large number of data points, scatterplots can become too dense to interpret. One solution is to bin the data and plot the bins. A common approach is hexagonal binning, where the plot is divided into hexagons, and the number of points in each hexagon is counted.\n",
    "\n",
    "- Hexagonal binning is a useful technique for visualizing the relationship between two numeric variables when the data is dense and a scatterplot is difficult to interpret.\n",
    "\n",
    "![Hexagonal Binning](01_00_hexbin.png)\n",
    "\n",
    "\n",
    "- Another approach is to use contour plots, which are similar to topographical maps. The density of the data is shown by the contours, with darker areas indicating higher density.\n",
    "\n",
    "- Contour plots are useful for visualizing the relationship between two numeric variables and identifying patterns in the data.\n",
    "\n",
    "![Contour Plot](01_00_contour.png)\n",
    "\n",
    "The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a “peak.”\n",
    "\n",
    "\n",
    "Other types of charts are used to show the relationship between two numeric variables, including heat maps. Heat maps, hexagonal binning, and contour plots all give a visual representation of a two-dimensional density. In this way, they are natural analogs to histograms and density plots.\n",
    "\n",
    "\n",
    "## Two Categorical Variables\n",
    "\n",
    "- For two categorical variables, a contingency table is a useful way to summarize the data. The table shows the counts of the data points that fall into each combination of categories.\n",
    "\n",
    "![Contingency Table](01_00_cont_table.png)\n",
    "\n",
    "\n",
    "Contingency tables can look only at counts, or they can also include column and total percentages. Pivot tables in Excel are perhaps the most common tool used to create contingency tables. The pandas library in Python also has a pivot_table method that can be used to create contingency tables.\n",
    "\n",
    "\n",
    "\n",
    "## Categorical and Numeric Data\n",
    "\n",
    "- When one variable is categorical and the other is numeric, a boxplot is a useful way to visualize the data. The boxplot shows the distribution of the numeric variable for each category of the categorical variable.\n",
    "\n",
    "![Boxplot](01_00_bplot.png)\n",
    "\n",
    "A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution\n",
    "that aren’t perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.\n",
    "\n",
    "\n",
    "![Violin Plot](01_00_violin.png)\n",
    "\n",
    "\n",
    "## Visualizing Multiple Variables\n",
    "\n",
    "- When you have more than two variables, it is often useful to visualize the relationships between multiple variables simultaneously. One common approach is to use a pair plot, which shows scatterplots of all pairs of variables in a dataset.\n",
    "\n",
    "- A pair plot is a grid of scatterplots showing the relationships between all pairs of variables in a dataset. It is a useful tool for visualizing the relationships between multiple variables and identifying patterns in the data.\n",
    "\n",
    "- The types of charts used to compare two variables—scatterplots, hexagonal binning, and boxplots—are readily extended to more variables through the notion of conditioning\n",
    "\n",
    "![faceting](01_00_facet.png)\n",
    "\n",
    "- Faceting is a technique that involves creating multiple plots, each showing a subset of the data based on a categorical variable. It is a useful way to compare the relationships between multiple variables across different categories.\n",
    "\n",
    "![Pair Plot](01_00_pair_plot.png)\n",
    "\n",
    "- Pair plots are a useful tool for visualizing the relationships between multiple variables in a dataset. They help to identify patterns, trends, and associations between variables and are commonly used in exploratory data analysis to understand the structure of the data.\n",
    "\n",
    "\n",
    "The concept of conditioning variables in a graphics system was pioneered with Trellis graphics, developed by Rick Becker, Bill Cleveland, and others at Bell Labs. This idea has propagated to various modern graphics systems, such as the lattice and ggplot2 packages in R and the seaborn and Bokeh modules in Python. Conditioning variables are also integral to business intelligence platforms such as Tableau and Spotfire. With the advent of vast computing power, modern visualization platforms have moved well beyond the humble beginnings of exploratory data analysis. However, key concepts and tools developed a half century ago (e.g., simple boxplots) still form a foundation for these systems.\n",
    "\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- Hexagonal binning and contour plots are useful tools that permit graphical\n",
    "examination of two numeric variables at a time, without being overwhelmed by\n",
    "huge amounts of data.\n",
    "\n",
    "- Contingency tables are the standard tool for looking at the counts of two categorical variables.\n",
    "\n",
    "- Boxplots and violin plots allow you to plot a numeric variable against a categorical variable.\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "Exploratory data analysis (EDA), pioneered by John Tukey, set a foundation for the field of data science. The key idea of EDA is that the first and most important step in\n",
    "any project based on data is to look at the data. By summarizing and visualizing the data, you can gain valuable intuition and understanding of the project. The concepts ranging from simple metrics, such as estimates of location and variability, to rich visual displays that explore the relationships between multiple variables. The diverse set of tools and techniques being developed by the open source community, combined with the expressiveness of the R and Python languages, has created a plethora of ways to explore and analyze data.\n",
    "Exploratory analysis should be a cornerstone of any data science project.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
